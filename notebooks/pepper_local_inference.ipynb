{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ü§ñ PersonaPlex: Local Control & Emotional System (Inside Out)\n",
                "\n",
                "This notebook allows you to run PersonaPlex locally without depending on Modal. \n",
                "\n",
                "**Features:**\n",
                "1. **CPU Offloading**: Allows running the 7B model on GPUs with low VRAM (like the T550).\n",
                "2. **Emotional Prompting**: System based on *Inside Out* to vary the tone (Joy, Sadness, etc.).\n",
                "3. **Local Text-to-Audio**: Generates and saves `.wav` files directly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "import IPython.display as ipd\n",
                "import sys\n",
                "import asyncio\n",
                "import wave\n",
                "\n",
                "# Add moshi path if not installed globally\n",
                "sys.path.append(os.path.abspath('moshi'))\n",
                "\n",
                "from moshi.models import loaders\n",
                "from sentencepiece import SentencePieceProcessor\n",
                "from moshi.models.lm import LMGen\n",
                "\n",
                "print(\"‚úÖ Libraries loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Model Initialization\n",
                "Loading Mimi (Audio) and Moshi LM (Brain)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "weights_dir = Path('weights')\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "\n",
                "print(f\"Loading models on {device} (this may take a few minutes)...\")\n",
                "\n",
                "# Mimi: The audio tokenizer\n",
                "mimi = loaders.get_mimi(weights_dir / loaders.MIMI_NAME, device)\n",
                "\n",
                "# Moshi LM: The language model (7B)\n",
                "# We use cpu_offload=True to handle GPUs with low VRAM\n",
                "moshi_lm = loaders.get_moshi_lm(\n",
                "    weights_dir / loaders.MOSHI_NAME, \n",
                "    device=device, \n",
                "    cpu_offload=True\n",
                ")\n",
                "\n",
                "tokenizer_path = weights_dir / loaders.TEXT_TOKENIZER_NAME\n",
                "text_tokenizer = SentencePieceProcessor(str(tokenizer_path))\n",
                "\n",
                "lm_gen = LMGen(moshi_lm, text_tokenizer)\n",
                "\n",
                "print(\"üöÄ System ready. GPU VRAM detected:\", torch.cuda.get_device_properties(0).total_memory / 1024**2 if device == 'cuda' else 'N/A', \"MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Emotional Configuration 'Inside Out'\n",
                "Injecting emotional state via specialized prompts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "EMOTIONS = {\n",
                "    \"Joy\": \"Hello! I'm so excited to talk to you! Hee-hee! Everything is wonderful! [laugh]\",\n",
                "    \"Sadness\": \"Hello... I'm feeling a bit down today. Everything seems so gray... [sigh]\",\n",
                "    \"Anger\": \"I can't believe it! This is unacceptable! Beep-boop-grrr! I'm very angry!\",\n",
                "    \"Fear\": \"What was that? I'm scared... are you there? Please don't go...\",\n",
                "    \"Disgust\": \"Ugh, how gross. That's repulsive. I don't even want to look at it. Puaj.\"\n",
                "}\n",
                "\n",
                "def wrap_with_system_tags(text):\n",
                "    \"\"\"Wrap text in tags the model expects for instructions.\"\"\"\n",
                "    return f\"(user) {text} (assistant)\"\n",
                "\n",
                "def get_emotional_prompt(emotion, user_text):\n",
                "    prefix = EMOTIONS.get(emotion, \"\")\n",
                "    return wrap_with_system_tags(f\"{prefix} {user_text}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Pepper's Voice Generation\n",
                "This cell runs the actual inference loop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_audio_local(text, emotion=\"Joy\", voice_pt=\"weights/pepper.pt\", output_wav=\"pepper_reply.wav\", duration_frames=300):\n",
                "    # 1. Load Pepper's voice identity\n",
                "    if os.path.exists(voice_pt):\n",
                "        lm_gen.load_voice_prompt_embeddings(voice_pt)\n",
                "        print(f\"‚úÖ Voice loaded from {voice_pt}\")\n",
                "    \n",
                "    # 2. Prepare emotional text prompt\n",
                "    emotional_text = get_emotional_prompt(emotion, text)\n",
                "    lm_gen.text_prompt_tokens = text_tokenizer.encode(emotional_text)\n",
                "    \n",
                "    print(f\"üé§ Pepper acting as: {emotion}...\")\n",
                "    all_audio_chunks = []\n",
                "    generated_text = \"\"\n",
                "    \n",
                "    # 4. Step-by-step generation loop (Streaming)\n",
                "    # Use zero audio codes for 'moshi_tokens' (empty input audio)\n",
                "    empty_audio_codes = torch.zeros((1, 8, 1), device=device, dtype=torch.long)\n",
                "    \n",
                "    for step in range(duration_frames):\n",
                "        # Model generates audio and text tokens simultaneously\n",
                "        tokens = lm_gen.step(moshi_tokens=empty_audio_codes)\n",
                "        \n",
                "        if tokens is not None:\n",
                "            # Audio tokens (indices 1 to 8 are for Mimi decoder)\n",
                "            audio_tokens = tokens[:, 1:9]\n",
                "            pcm_chunk = mimi.decode(audio_tokens)\n",
                "            all_audio_chunks.append(pcm_chunk.cpu().numpy().flatten())\n",
                "            \n",
                "            # Text token (index 0)\n",
                "            text_token = tokens[0, 0, 0].item()\n",
                "            if text_token > 3: # Ignore special tokens (padding, etc)\n",
                "                piece = text_tokenizer.id_to_piece(text_token).replace('‚ñÅ', ' ')\n",
                "                generated_text += piece\n",
                "                if step % 20 == 0: print(f\"Text: {generated_text}\")\n",
                "\n",
                "    # 5. Concatenate and save audio\n",
                "    full_audio = np.concatenate(all_audio_chunks)\n",
                "    \n",
                "    # Save to .wav file\n",
                "    import scipy.io.wavfile as wavfile\n",
                "    wavfile.write(output_wav, 24000, (full_audio * 32767).astype(np.int16))\n",
                "    \n",
                "    print(f\"\\n‚ú® Pepper's reply finished.\")\n",
                "    return output_wav, generated_text\n",
                "\n",
                "# --- LOCAL TEST ---\n",
                "# wav_file, text_out = generate_audio_local(\"Hello Pepper! How are you today?\", emotion=\"Joy\")\n",
                "# ipd.Audio(wav_file)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}